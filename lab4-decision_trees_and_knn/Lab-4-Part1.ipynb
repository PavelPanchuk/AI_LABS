{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4. Часть 1. Деревья принятия решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем обзор методов классификации и регрессии с одного из самых популярных – с дерева решений. Деревья решений используются в повседневной жизни в самых разных областях человеческой деятельности, порой и очень далеких от машинного обучения. Деревом решений можно назвать наглядную инструкцию, что делать в какой ситуации. \n",
    "\n",
    "Зачастую дерево решений служит обобщением опыта экспертов, средством передачи знаний будущим сотрудникам или моделью бизнес-процесса компании. Например, до внедрения масштабируемых алгоритмов машинного обучения в банковской сфере задача **кредитного скоринга** решалась экспертами. Решение о выдаче кредита заемщику принималось на основе некоторых интуитивно (или по опыту) выведенных правил, которые можно представить в виде дерева решений.\n",
    "<img src=\"https://habrastorage.org/files/194/9b6/ae9/1949b6ae97ab4fc9b1a37fbf182eda8f.gif\"/><br>\n",
    "В этом случае можно сказать, что решается задача бинарной классификации (целевой класс имеет два значения: \"Выдать кредит\" и \"Отказать\") по признакам \"Возраст\", \"Наличие дома\", \"Доход\" и \"Образование\".\n",
    "\n",
    "Дерево решений как алгоритм машинного обучения – по сути то же самое: объединение логических правил вида \"Значение признака $a$ меньше $x$ И Значение признака $b$ меньше $y$ ... => Класс 1\" в структуру данных \"Дерево\". **Огромное преимущество деревьев решений** в том, что они легко интерпретируемы, понятны человеку. Например, по схеме на рисунке выше можно объяснить заемщику, почему ему было отказано в кредите. Скажем, потому, что у него нет дома и доход меньше 5000. Многие другие, хоть и более точные, модели не обладают этим свойством и могут рассматриваться скорее как \"черный ящик\", в который загрузили данные и получили ответ. В связи с этой **\"понятностью\"** деревьев решений и их сходством с моделью принятия решений человеком (можно легко объяснять боссу свою модель), деревья решений получили огромную популярность, а один из представителей этой группы методов классификации, С4.5, рассматривается первым в списке 10 лучших алгоритмов интеллектуального анализа данных (\"Top 10 algorithms in data mining\", Knowledge and Information Systems, 2008. [PDF](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как строится дерево решений\n",
    "\n",
    "В примере с кредитным скорингом мы видели, что решение о выдаче кредита принималось на основе возраста, наличия недвижимости, дохода и других. Но какой признак выбрать первым?  \n",
    "\n",
    "**Основной принцип:** выбираем признак, который наилучшим образом разделит выборку.\n",
    "\n",
    "Здесь можно вспомнить игру \"20 вопросов\", которая часто упоминается во введении в деревья решений. Наверняка каждый в нее играл. Один человек загадывает знаменитость, а второй пытается отгадать, задавая только вопросы, на которые можно ответить \"Да\" или \"Нет\" (опустим варианты \"не знаю\" и \"не могу сказать\"). Какой вопрос отгадывающий задаст первым делом? Конечно, такой, который сильнее всего уменьшит количество оставшихся вариантов. К примеру, вопрос \"Это Анджелина Джоли?\" в случае отрицательного ответа оставит более 6 миллиардов вариантов для дальнейшего перебора (конечно, поменьше, не каждый человек – знаменитость, но все равно немало), а вот вопрос \"Это женщина?\" отсечет уже около половины знаменитостей. То есть, признак \"пол\" намного лучше разделяет выборку людей, чем признак \"это Анджелина Джоли\", \"национальность-испанец\" или \"любит футбол\". Это интуитивно соответствует понятию прироста информации, основанного на энтропии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Энтропия\n",
    "Энтропия Шеннона определяется для системы с $N$ возможными состояниями следующим образом:\n",
    "\n",
    "$$S = -\\sum_{i=1}^{N}p_ilog_2p_i,$$\n",
    "\n",
    "где  $p_i$ – вероятности нахождения системы в $i$-ом состоянии. Энтропия соответствует степени упорядоченности (\"хаотичности\") в системе. Чем выше энтропия, тем менее упорядочена система и наоборот.\n",
    "\n",
    "<h4>Пример</h4>\n",
    "Для иллюстрации того, как энтропия поможет определить хорошие признаки для построения дерева, приведем тот же игрушечный пример, что в статье <a href=\"https://habrahabr.ru/post/171759/\">\"Энтропия и деревья принятия решений\"</a>. Будем предсказывать цвет шарика по его координате. Конечно, ничего общего с жизнью это не имеет, но позволяет показать, как энтропия используется для построения дерева решений.\n",
    "<img src=\"https://habrastorage.org/files/c96/80a/a4b/c9680aa4babc40f4bbc8b3595e203979.png\"/><br>\n",
    "\n",
    "Здесь 9 синих шариков и 11 желтых. Если мы наудачу вытащили шарик, то он с вероятностью   $p_1=\\frac{9}{20}$ будет синим и с вероятностью  $p_2=\\frac{11}{20}$ – желтым. Значит, энтропия состояния  $S_0 = -\\frac{9}{20}log_2{\\frac{9}{20}}-\\frac{11}{20}log_2{\\frac{11}{20}} \\approx 1$. Само это значение пока ни о чем нам не говорит. Теперь посмотрим, как изменится энтропия, если разбить шарики на две группы – с координатой меньше либо равной 12 и больше 12.\n",
    "<img src=\"https://habrastorage.org/files/186/444/a8b/186444a8bd0e451c8324ca8529f8d4f4.png\"/><br>\n",
    "\n",
    "В левой группе оказалось 13 шаров, из которых 8 синих и 5 желтых. Энтропия этой группы равна $S_1 = -\\frac{5}{13}log_2{\\frac{5}{13}}-\\frac{8}{13}log_2{\\frac{8}{13}} \\approx 0.96$. В правой группе оказалось 7 шаров, из которых 1 синий и 6 желтых. Энтропия правой группы равна $S_2 = -\\frac{1}{7}log_2{\\frac{1}{7}}-\\frac{6}{7}log_2{\\frac{6}{7}} \\approx 0.6$. Как видим, энтропия уменьшилась в обеих группах по сравнению с начальным состоянием, хоть в левой и не сильно. Поскольку энтропия – по сути степень хаоса (или неопределенности) в системе, уменьшение энтропии называют приростом информации. \n",
    "\n",
    "Формально **прирост информации (information gain, IG)** при разбиении выборки по признаку $Q$ (в нашем примере это признак \"$x \\leq 12$\") определяется как \n",
    "$$IG(Q) = S_O - \\sum_{i=1}^{q}\\frac{|N_i|}{N}S_i,$$\n",
    "где $q$ – число групп после разбиения, $N_i$ – число элементов в $i$-й группе(в которой признак $Q$ имеет $i$-ое значение), $S_i$ - энтропия этой группы. \n",
    "\n",
    "В нашем случае после разделения получилось две группы ($q = 2$) – одна из 13 элементов ($N_1 = 13$), вторая – из 7 ($N_2 = 7$). Прирост информации получился \n",
    "$$IG(\"x \\leq 12\") = S_0 - \\frac{13}{20}S_1 - \\frac{7}{20}S_2 \\approx 0.16.$$\n",
    "Получается, разделив шарики на две группы по признаку \"координата меньше либо равна 12\", мы уже получили более упорядоченную систему, чем в начале. Продолжим деление шариков на группы до тех пор, пока в каждой группе шарики не будут одного цвета.  \n",
    "<img src=\"https://habrastorage.org/files/dae/a88/2b0/daea882b0a8e4ef4b23325c88f0353a1.png\"/><br>\n",
    "Для правой группы потребовалось всего одно дополнительное разбиение по признаку \"координата меньше либо равна 18\", для левой – еще три. Очевидно, энтропия группы с шариками одного цвета равна 0 ($log_2{1} = 0$), что соответствует представлению, что группа шариков одного цвета – упорядоченная. \n",
    "В итоге мы построили дерево решений, предсказывающее цвет шарика по его координате. Такое дерево решений может плохо работать для новых объектов (определения цвета новых шариков), поскольку оно идеально подстроилось под обучающую выборку (изначальные 20 шариков). Для классификации новых шариков лучше подойдет дерево с меньшим числом \"вопросов\", или разделений, пусть даже оно и не идеально разбивает по цветам обучающую выборку. Это как раз ситуация переобучения, о которой упоминалось на лекциях. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм построения дерева\n",
    "\n",
    "Можно убедиться в том, что построенное в предыдущем примере дерево является в некотором смысле оптимальным – потребовалось только 5 \"вопросов\" (условий на признак $x$), чтобы \"подогнать\" дерево решений под обучающую выборку, то есть чтобы дерево правильно классифицировало любой обучающий объект. При других условиях разделения выборки дерево получится глубже. \n",
    "\n",
    "В основе популярных алгоритмов построения дерева решений, таких как ID3 и C4.5, лежит принцип жадной максимизации прироста информации – на каждом шаге выбирается тот признак, при разделении по которому прирост информации оказывается наибольшим. Дальше процедура повторяется рекурсивно, пока энтропия не окажется равной нулю или какой-то малой величине (если дерево не подгоняется идеально под обучающую выборку во избежание переобучения).\n",
    "В разных алгоритмах применяются разные эвристики для \"ранней остановки\" или \"отсечения\", чтобы избежать построения переобученного дерева. \n",
    "\n",
    "```python\n",
    "def build(L):\n",
    "    create node t\n",
    "    if the stopping criterion is True:\n",
    "        assign a predictive model to t\n",
    "    else:\n",
    "        Find the best binary split L = L_left + L_right\n",
    "        t.left = build(L_left)\n",
    "        t.right = build(L_right)\n",
    "    return t     \n",
    "```\n",
    "\n",
    "### Другие критерии качества разбиения в задаче классификации\n",
    "\n",
    "Мы разобрались, в том, как понятие энтропии позволяет формализовать представление о качестве разбиения в дереве. Но это всего-лишь эвристика, существуют и другие:\n",
    " - **Неопределенность Джини (Gini impurity):**  $G = 1 - \\sum\\limits_k (p_k)^2$;\n",
    " - **Ошибка классификации (misclassification error):**  $E = 1 - \\max\\limits_k p_k$;\n",
    "  \n",
    "Максимизация неопределенности Джини можно интерпретировать как максимизацию числа пар объектов одного класса, оказавшихся в одном поддереве. Неопределенность Джини и прирост информации работают почти одинаково. Ошибка классификации на практике почти не используется.\n",
    "\n",
    "В случае задачи бинарной классификации ($p_+$ – вероятность объекта иметь метку +) энтропия и неопределенность Джини примут следующий вид:<br><br>\n",
    "$$ S = -p_+ \\log_2{p_+} -p_- \\log_2{p_-} = -p_+ \\log_2{p_+} -(1 - p_{+}) \\log_2{(1 - p_{+})};$$\n",
    "\n",
    "$$ G = 1 - p_+^2 - p_-^2 = 1 - p_+^2 - (1 - p_+)^2 = 2p_+(1-p_+).$$\n",
    "\n",
    "Когда мы построим графики этух двух функций от аргумента $p_+$, то увидим, что график энтропии очень близок к графику удвоенной неопределенности Джини, и поэтому на практике эти два критерия \"работают\" почти одинаково."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "xx = np.linspace(0,1,50)\n",
    "plt.plot(xx, [2 * x * (1-x) for x in xx], label='gini')\n",
    "plt.plot(xx, [4 * x * (1-x) for x in xx], label='2*gini')\n",
    "plt.plot(xx, [-x * np.log2(x) - (1-x) * np.log2(1 - x)  for x in xx], label='entropy')\n",
    "plt.plot(xx, [1 - max(x, 1-x) for x in xx], label='missclass')\n",
    "plt.plot(xx, [2 - 2 * max(x, 1-x) for x in xx], label='2*missclass')\n",
    "plt.xlabel('p+')\n",
    "plt.ylabel('criterion')\n",
    "plt.title('Критерии качества как функции от p+ (бинарная классификация)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример применения деревьев на синтетических данных\n",
    "Рассмотрим пример применения дерева решений из библиотеки Scikit-learn. Сгенерируем данные для бинарной классификации. Два класса будут сгенерированы из двух нормальных распределений с разными средними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем генератор случайных чисел для повторяемости\n",
    "np.seed = 7\n",
    "\n",
    "# первый класс\n",
    "train_data = np.random.normal(size=(100, 2))\n",
    "train_labels = np.zeros(100)\n",
    "\n",
    "# добавляем второй класс\n",
    "train_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)]\n",
    "train_labels = np.r_[train_labels, np.ones(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вспомогательную функцию, которая будет возвращать решетку для дальнейшей красивой визуализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(data, eps=0.01):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, eps),\n",
    "                         np.arange(y_min, y_max, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим данные. Неформально, задача классификации в этом случае – построить какую-то \"хорошую\" границу, разделяющую 2 класса (красные точки от желтых). Если утрировать, то машинное обучение в этом случае сводится к тому, как выбрать хорошую разделяющую границу. Возможно,  прямая будет слишком простой границей, а какая-то сложная кривая, огибающая каждую красную точку – будет слишком сложной и будем много ошибаться на новых примерах из того же распределения, из которого пришла обучающая выборка. Интуиция подсказывает, что хорошо на новых данных будет работать какая-то *гладкая* граница, разделяющая 2 класса, или хотя бы просто прямая (в $n$-мерном случае - гиперплоскость). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,8)\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, \n",
    "            cmap='autumn', edgecolors='black', linewidth=1.5)\n",
    "plt.plot(range(-2,5), range(4,-3,-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разделить эти два класса, обучив дерево решений. В дереве будем использовать параметр `max_depth`, ограничивающий глубину дерева. Визуализируем полученную границу разделения класссов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# параметр min_samples_leaf указывает, при каком минимальном количестве\n",
    "# элементов в узле он будет дальше разделяться\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=17)\n",
    "\n",
    "# обучаем дерево\n",
    "clf_tree.fit(train_data, train_labels)\n",
    "\n",
    "# немного кода для отображения разделяющей поверхности\n",
    "xx, yy = get_grid(train_data)\n",
    "predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, predicted, cmap='autumn')\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, \n",
    "            cmap='autumn', edgecolors='black', linewidth=1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем .dot формат для визуализации дерева\n",
    "# возможно понадобится установить библиотеку pydotplus\n",
    "# и ПО для визуализации GraphViz\n",
    "# Возможно вам пригодится ссылка\n",
    "# https://datascience.stackexchange.com/questions/37428/graphviz-and-pydotplus-not-working\n",
    "\n",
    "from ipywidgets import Image\n",
    "from io import StringIO\n",
    "import pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf_tree, feature_names=['x1', 'x2'], \n",
    "                out_file=dot_data, filled=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(value=graph.create_jpg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате у вас в GraphViz должно быть построено дерево вида:\n",
    "<img src=\"img/synthetic_dt.jpeg\">\n",
    "\n",
    "Как \"читается\" такое дерево?\n",
    "\n",
    "В начале было 200 объектов, 100 - одного класса и 100 – другого. Энтропия начального состояния была максимальной – 1. Затем было сделано разбиение объектов на 2 группы в зависимости от сравнения признака $x_1$ со значением $0.3631$ (найдите этот участок границы на рисунке выше, до дерева). При этом энтропия и в левой, и в правой группе объектов уменьшилась. И так далее, дерево строится до глубины 3. При такой визуализации чем больше объектов одного класса, тем  цвет вершины ближе к темно-оранжевому и, наоборот, чем больше объектов второго класса, тем ближе цвет к темно-синему. В начале объектов одного лкасса поровну, поэтому корневая вершина дерева – белого цвета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как дерево решений работает с количественными признаками\n",
    "\n",
    "Допустим, в выборке имеется количественный признак \"Возраст\", имеющий много уникальных значений. Дерево решений будет искать лучшее (по критерию типа прироста информации) разбиение выборки, проверяя бинарные признаки типа \"Возраст < 17\", \"Возраст < 22.87\" и т.д. Но что если таких \"нарезаний\" возраста слишком много? А что если есть еще количественный признак \"Зарплата\", и зарплату тоже можно \"нарезать\" большим числом способов? Получается слишком много бинарных признаков для выбора лучшего на каждом шаге построения дерева. Для решения этой проблемы применяют эвристики для ограничения числа порогов, с которыми мы сравниваем количественный признак. \n",
    "\n",
    "Рассмотрим это на игрушечном примере. Пусть есть следующая выборка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Возраст': [17,64,18,20,38,49,55,25,29,31,33], \n",
    "             'Невозврат кредита': [1,0,1,0,1,0,0,1,1,0,1]})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем ее по возрастанию возраста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('Возраст')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим на этих данных дерево решений (без ограничения глубины) и посмотрим на него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tree = DecisionTreeClassifier(random_state=17)\n",
    "age_tree.fit(data['Возраст'].values.reshape(-1, 1), data['Невозврат кредита'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(age_tree, feature_names=['Возраст'], \n",
    "                out_file=dot_data, filled=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(value=graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что дерево задействовало 5 значений, с которыми сравнивается возраст: 43.5, 19, 22.5, 30 и 32 года. Если приглядеться, то это средние значения между возрастами, при которых целевой класс \"меняется\" с 1 на 0 или наоборот. То есть в качестве порогов для \"нарезания\" количественного признака, дерево \"смотрит\" на те значения, при которых целевой класс меняет свое значение. Подумайте, почему не имеет смысла в данном случае рассматривать признак \"Возраст < 17.5\".\n",
    "\n",
    "Рассмотрим пример посложнее: добавим признак \"Зарплата\" (тыс. рублей/месяц)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame({'Возраст':  [17,64,18,20,38,49,55,25,29,31,33], \n",
    "                      'Зарплата': [25,80,22,36,37,59,74,70,33,102,88], \n",
    "             'Невозврат кредита': [1,0,1,0,1,0,0,1,1,0,1]})\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если отсортировать по возрасту, то целевой класс (\"Невозврат кредита\") меняется (с 1 на 0 или наоборот) 5 раз. А если отсортировать по зарплате – то 7 раз. Как теперь дерево будет выбирать признаки? Посмотрим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.sort_values('Возраст')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.sort_values('Зарплата')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sal_tree = DecisionTreeClassifier(random_state=17)\n",
    "age_sal_tree.fit(data2[['Возраст', 'Зарплата']].values, data2['Невозврат кредита'].values);\n",
    "dot_data = StringIO()\n",
    "export_graphviz(age_sal_tree, feature_names=['Возраст', 'Зарплата'], \n",
    "                out_file=dot_data, filled=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(value=graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что в дереве задействованы как разбиения по возрасту, так и по зарплате. Причем пороги, с которыми сравниваются признаки: 43.5 и 22.5 года – для возраста и 95 и 30.5 тыс. руб/мес – для зарплаты. И опять можно заметить, что 95 тыс. – это среднее между 88 и 102, при этом человек с зарплатой 88 оказался \"плохим\", а с 102 – \"хорошим\". То же самое для 30.5 тыс. То есть перебирались сравнения зарплаты и возраста не со всеми возможными значениями, а только с несколькими. А почему в дереве оказались именно эти признаки? Потому что по ним разбиения оказались лучше (по критерию неопределенности Джини).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** самая простая эвристика для обработки количественных признаков в дереве решений: количественный признак сортируется по возрастанию, и в дереве проверяются только те пороги, при которых целевой признак меняет значение.\n",
    "\n",
    "**Дополнительно:** когда в данных много количественных признаков, и у каждого много уникальных значений, могут отбираться не все пороги, описанные выше, а только топ-N, дающих максимальный прирост все того же критерия. То есть, по сути, для каждого порога строится дерево глубины 1, считается насколько снизилась энтропия (или неопределенность Джини) и выбираются только лучшие пороги, с которыми стоит сравнивать количественный признак. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные параметры дерева\n",
    "\n",
    "В принципе дерево решений можно построить до такой глубины, чтоб в каждом листе был ровно один объект. Но на практике это не делается из-за того, что такое дерево будет *переобученным* – оно слишком настроится на обучающую выборку и будет плохо работать на прогноз на новых данных. Где-то внизу дерева, на большой глубине будут появляться разбиения по менее важным признакам. Если утрировать, может оказаться так, что из всех 4 клиентов, пришедших в банк за кредитом в зеленых штанах, никто не вернул кредит. Но мы не хотим, чтобы наша модель классификации порождала такие специфичные правила. \n",
    "\n",
    "Основные способы борьбы с переобучением в случае деревьев решений:\n",
    " - искусственное ограничение глубины или минимального числа объектов в листе: построение дерева просто в какой-то момент прекращается;\n",
    " - стрижка дерева (*pruning*). При таком подходе дерево сначала строится до максимальной глубины, потом постепенно, снизу вверх, некоторые вершины дерева убираются за счет сравнения по качеству дерева с данным разбиением и без него (сравнение проводится с помощью *кросс-валидации*, о которой чуть ниже). Подробнее можно почитать в  материалах [репозитория](https://github.com/esokolov/ml-course-msu) Евгения Соколова.\n",
    "\n",
    "### Класс DecisionTreeClassifier в Scikit-learn\n",
    "Основные параметры класса [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):\n",
    "\n",
    "- `max_depth` – максимальная глубина дерева\n",
    "- `max_features` - максимальное число признаков, по которым ищется лучшее разбиение в дереве (это нужно потому, что при большом количестве признаков будет \"дорого\" искать лучшее (по критерию типа прироста информации) разбиение среди *всех* признаков)\n",
    "- `min_samples_leaf` – минимальное число объектов в листе. У этого параметра есть понятная интерпретация: скажем, если он равен 5, то дерево будет порождать только те классифицирующие правила, которые верны как мимимум для 5 объектов\n",
    "\n",
    "Параметры дерева надо настраивать в зависимости от входных данных, и делается это обычно с помощью *кросс-валидации*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дерево решений в задаче регрессии\n",
    "\n",
    "При прогнозировании количественного признака идея построения дерева остается та же, но меняется критерий качества. Используется **дисперсия вокруг среднего:**\n",
    "\n",
    "$$D = \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} (y_i - \\frac{1}{\\ell} \\sum\\limits_{j=1}^{\\ell} y_j)^2, $$\n",
    "где $\\ell$ – число объектов в листе, $y_i$ – значения целевого признака. Попросту говоря, минимизируя дисперсию вокруг среднего, мы ищем признаки, разбивающие выборку таким образом, что значения целевого признака в каждом листе примерно равны.\n",
    "\n",
    "#### Пример\n",
    "Сгенерируем данные, распределенные вокруг функции $f(x) = e^{-x ^ 2} + 1.5 * e^{-(x - 2) ^ 2}$ c некоторым шумом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 150        \n",
    "n_test = 1000       \n",
    "noise = 0.1\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2) + \\\n",
    "        np.random.normal(0.0, noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "     \n",
    "reg_tree = DecisionTreeRegressor(max_depth=5, random_state=17)\n",
    "\n",
    "reg_tree.fit(X_train, y_train)\n",
    "reg_tree_pred = reg_tree.predict(X_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, reg_tree_pred, \"g\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Decision tree regressor, MSE = %.2f\" % (np.sum((y_test - reg_tree_pred) ** 2) / n_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использованы материалы **\"Открытого курса по машинному обучению\"**. Авторы: программист-исследователь Mail.ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ Юрий Кашницкий и Data Scientist в Segmento Екатерина Демидова. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
